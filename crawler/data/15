
http://old-www.cs.dartmouth.edu/~cs50/data/tse/wikipedia/wiki/CPU_cache.html
2
CPU cache Wikipedia the free encyclopedia a lang ar a lang kk arab a lang mzn a lang ps a lang ur text decoration none cache key enwiki resourceloader filter minify css d a aa f a dc f be e if window mw mw config set wgCanonicalNamespace wgCanonicalSpecialPageName false wgNamespaceNumber wgPageName CPU cache wgTitle CPU cache wgCurRevisionId wgRevisionId wgArticleId wgIsArticle true wgIsRedirect false wgAction view wgUserName null wgUserGroups wgCategories All articles with dead external links Articles with dead external links from July Articles to be merged from July All articles to be merged All articles with unsourced statements Articles with unsourced statements from March All accuracy disputes Articles with disputed statements from December Central processing unit Computer memory Cache computing wgBreakFrames false wgPageContentLanguage en wgPageContentModel wikitext wgSeparatorTransformTable wgDigitTransformTable wgDefaultDateFormat dmy wgMonthNames January February March April May June July August September October November December wgMonthNamesShort Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec wgRelevantPageName CPU cache wgIsProbablyEditable true wgRestrictionEdit wgRestrictionMove wgWikiEditorEnabledModules toolbar true dialogs true hidesig true templateEditor false templates false preview false previewDialog false publish false toc false wgBetaFeaturesFeatures wgVisualEditor isPageWatched false magnifyClipIconURL bits wikimedia org static wmf skins common images magnify clip png pageLanguageCode en pageLanguageDir ltr svgMaxSize wikilove recipient wikilove anon wgGuidedTourHelpGuiderUrl Help Guided tours guider wgFlowTermsOfUseEdit By saving changes you agree to our u Ca class external text href wikimediafoundation org wiki Terms of use u ETerms of Use u C a u E and agree to irrevocably release your text under the u Ca rel nofollow class external text href creativecommons org licenses by sa u ECC BY SA License u C a u E and u Ca class external text href en wikipedia org wiki Wikipedia Text of the GNU Free Documentation License u EGFDL u C a u E wgULSAcceptLanguageList en gb wgULSCurrentAutonym English wgFlaggedRevsParams tags status levels quality pristine wgStableRevisionId null wgCategoryTreePageCategoryOptions mode hideprefix showcount true namespaces false wgNoticeProject wikipedia wgWikibaseItemId Q if window mw mw loader implement user options function mw user options set ccmeonemails cols date default diffonly disablemail editfont default editondblclick editsectiononrightclick enotifminoredits enotifrevealaddr enotifusertalkpages enotifwatchlistpages extendwatchlist fancysig forceeditsummary gender unknown hideminor hidepatrolled imagesize math minordefault newpageshidepatrolled nickname norollbackdiff numberheadings previewonfirst previewontop rcdays rclimit rememberpassword rows showhiddencats false shownumberswatching showtoolbar skin vector stubthreshold thumbsize underline uselivepreview usenewrc watchcreations watchdefault watchdeletion watchlistdays watchlisthideanons watchlisthidebots watchlisthideliu watchlisthideminor watchlisthideown watchlisthidepatrolled watchmoves wllimit useeditwarning prefershttps flaggedrevssimpleui flaggedrevsstable flaggedrevseditdiffs true flaggedrevsviewdiffs false usebetatoolbar usebetatoolbar cgd visualeditor enable visualeditor enable experimental visualeditor betatempdisable wikilove enabled echo subscriptions web page review true echo subscriptions email page review false ep showtoplink false ep bulkdelorgs false ep bulkdelcourses true ep showdyk true echo subscriptions web education program true echo subscriptions email education program false echo notify show link true echo show alert true echo email frequency echo email format html echo subscriptions email system true echo subscriptions web system true echo subscriptions email other false echo subscriptions web other true echo subscriptions email edit user talk false echo subscriptions web edit user talk true echo subscriptions email reverted false echo subscriptions web reverted true echo subscriptions email article linked false echo subscriptions web article linked false echo subscriptions email mention false echo subscriptions web mention true echo subscriptions web edit thank true echo subscriptions email edit thank false echo subscriptions web flow discussion true echo subscriptions email flow discussion false gettingstarted task toolbar show intro true uls preferences language en variant gan gan variant iu iu variant kk kk variant ku ku variant shi shi variant sr sr variant tg tg variant uz uz variant zh zh searchNs true searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false searchNs false gadget teahouse gadget ReferenceTooltips gadget DRN wizard gadget charinsert gadget mySandbox variant en mw loader implement user tokens function mw user tokens set editToken patrolToken false watchToken false cache key enwiki resourceloader filter minify js e d ede d d dc cd if window mw mw loader load mediawiki page startup mediawiki legacy wikibits mediawiki legacy ajax ext centralauth centralautologin skins vector compactPersonalBar defaultTracking ext visualEditor viewPageTarget init ext uls init ext uls interface wikibase client init ext centralNotice bannerController skins vector js body behavior url w static wmf skins vector csshover min htc CPU cache From Wikipedia the free encyclopedia Jump to navigation search Cache memory redirects here For the general use in computing see Cache computing It has been suggested that Tag RAM be merged into this article Discuss Proposed since July A CPU cache is a cache used by the central processing unit CPU of a computer to reduce the average time to access memory The cache is a smaller faster memory which stores copies of the data from frequently used main memory locations Most CPUs have different independent caches including instruction and data caches where the data cache is usually organized as a hierarchy of more cache levels L L etc Contents Overviews Cache entries Cache performance Replacement policies Write policies CPU stalls Cache entry structure Example Flag bits Associativity Direct mapped cache Two way set associative cache Speculative execution Two way skewed associative cache Pseudo associative cache Cache miss Address translation Homonym and synonym problems Virtual tags and vhints Page coloring Cache hierarchy in a modern processor Specialized caches Victim cache Trace cache Micro operation uop cache Multi level caches Multi core chips Separate versus unified Exclusive versus inclusive Example the K More hierarchies Implementation History First TLB implementations First data cache In x microprocessors Current research Multi ported cache See also Notes References External links Overviews edit When the processor needs to read from or write to a location in main memory it first checks whether a copy of that data is in the cache If so the processor immediately reads from or writes to the cache which is much faster than reading from or writing to main memory Most modern desktop and server CPUs have at least three independent caches an instruction cache to speed up executable instruction fetch a data cache to speed up data fetch and store and a translation lookaside buffer TLB used to speed up virtual to physical address translation for both executable instructions and data The data cache is usually organized as a hierarchy of more cache levels L L etc see Multi level caches Cache entries edit Data is transferred between memory and cache in blocks of fixed size called cache lines When a cache line is copied from memory into the cache a cache entry is created The cache entry will include the copied data as well as the requested memory location now called a tag When the processor needs to read or write a location in main memory it first checks for a corresponding entry in the cache The cache checks for the contents of the requested memory location in any cache lines that might contain that address If the processor finds that the memory location is in the cache a cache hit has occurred However if the processor does not find the memory location in the cache a cache miss has occurred In the case of a cache hit the processor immediately reads or writes the data in the cache line a cache miss the cache allocates a new entry and copies in data from main memory then the request is fulfilled from the contents of the cache Cache performance edit The proportion of accesses that result in a cache hit is known as the hit rate and can be a measure of the effectiveness of the cache for a given program or algorithm Read misses delay execution because they require data to be transferred from memory much more slowly than the cache itself Write misses may occur without such penalty since the processor can continue execution while data is copied to main memory in the background Replacement policies edit Main article Cache algorithms In order to make room for the new entry on a cache miss the cache may have to evict one of the existing entries The heuristic that it uses to choose the entry to evict is called the replacement policy The fundamental problem with any replacement policy is that it must predict which existing cache entry is least likely to be used in the future Predicting the future is difficult so there is no perfect way to choose among the variety of replacement policies available One popular replacement policy least recently used LRU replaces the least recently accessed entry Marking some memory ranges as non cacheable can improve performance by avoiding caching of memory regions that are rarely re accessed This avoids the overhead of loading something into the cache without having any reuse Cache entries may also be disabled or locked depending on the context Write policies edit Main article Cache Writing policies If data is written to the cache at some point it must also be written to main memory The timing of this write is known as the write policy In a write through cache every write to the cache causes a write to main memory Alternatively in a write back or copy back cache writes are not immediately mirrored to the main memory Instead the cache tracks which locations have been written over these locations are marked dirty The data in these locations are written back to the main memory only when that data is evicted from the cache For this reason a read miss in a write back cache may sometimes require two memory accesses to service one to first write the dirty location to memory and then another to read the new location from memory There are intermediate policies as well The cache may be write through but the writes may be held in a store data queue temporarily usually so that multiple stores can be processed together which can reduce bus turnarounds and improve bus utilization The data in main memory being cached may be changed by other entities e g peripherals using direct memory access or multi core processor in which case the copy in the cache may become out of date or stale Alternatively when a CPU in a multiprocessor system updates data in the cache copies of data in caches associated with other CPUs will become stale Communication protocols between the cache managers which keep the data consistent are known as cache coherence protocols CPU stalls edit The time taken to fetch one cache line from memory read latency matters because the CPU will run out of things to do while waiting for the cache line When a CPU reaches this state it is called a stall As CPUs become faster stalls due to cache misses displace more potential computation modern CPUs can execute hundreds of instructions in the time taken to fetch a single cache line from main memory Various techniques have been employed to keep the CPU busy during this time Out of order CPUs the Pentium Pro and later Intel designs for example attempt to execute independent instructions after the instruction that is waiting for the cache miss data Another technology used by many processors is simultaneous multithreading SMT or in Intel s terminology hyper threading HT which allows an alternate thread to use the CPU core while a first thread waits for data to come from main memory Cache entry structure edit Cache row entries usually have the following structure tag data block flag bits The data block cache line contains the actual data fetched from the main memory The tag contains part of the address of the actual data fetched from the main memory The flag bits are discussed below The size of the cache is the amount of main memory data it can hold This size can be calculated as the number of bytes stored in each data block times the number of blocks stored in the cache The number of tag and flag bits is irrelevant to this calculation although it does affect the physical area of a cache An effective memory address is split MSB to LSB into the tag the index and the block offset tag index block offset The index describes which cache row which cache line that the data has been put in The index length is bits for r cache rows The block offset specifies the desired data within the stored data block within the cache row Typically the effective address is in bytes so the block offset length is bits where b is the number of bytes per data block The tag contains the most significant bits of the address which are checked against the current row the row has been retrieved by index to see if it is the one we need or another irrelevant memory location that happened to have the same index bits as the one we want The tag length in bits is address length index length block offset length Some authors refer to the block offset as simply the offset or the displacement Example edit The original Pentium processor had a four way set associative L data cache of KB in size with byte cache blocks Hence there are KB cache blocks This implies sets and hence different indices There are possible offsets Since the CPU address is bits wide this implies and hence bits for the tag field The original Pentium processor also had an eight way set associative L integrated cache KB in size with byte cache blocks This implies and hence bits for the tag field Flag bits edit An instruction cache requires only one flag bit per cache row entry a valid bit The valid bit indicates whether or not a cache block has been loaded with valid data On power up the hardware sets all the valid bits in all the caches to invalid Some systems also set a valid bit to invalid at other times such as when multi master bus snooping hardware in the cache of one processor hears an address broadcast from some other processor and realizes that certain data blocks in the local cache are now stale and should be marked invalid A data cache typically requires two flag bits per cache row entry a valid bit and also a dirty bit The dirty bit indicates whether that block has been unchanged since it was read from main memory clean or whether the processor has written data to that block and the new value has not yet made it all the way to main memory dirty Associativity edit Which memory locations can be cached by which cache locations The replacement policy decides where in the cache a copy of a particular entry of main memory will go If the replacement policy is free to choose any entry in the cache to hold the copy the cache is called fully associative At the other extreme if each entry in main memory can go in just one place in the cache the cache is direct mapped Many caches implement a compromise in which each entry in main memory can go to any one of N places in the cache and are described as N way set associative For example the level data cache in an AMD Athlon is two way set associative which means that any particular location in main memory can be cached in either of two locations in the level data cache Associativity is a trade off If there are ten places to which the replacement policy could have mapped a memory location then to check if that location is in the cache ten cache entries must be searched Checking more places takes more power chip area and potentially time On the other hand caches with more associativity suffer fewer misses see conflict misses below so that the CPU wastes less time reading from the slow main memory The rule of thumb is that doubling the associativity from direct mapped to two way or from two way to four way has about the same effect on hit rate as doubling the cache size Associativity increases beyond four way have much less effect on the hit rate and are generally done for other reasons see virtual aliasing below In order of worse but simple to better but complex direct mapped cache the best fastest hit times and so the best tradeoff for large caches two way set associative cache two way skewed associative cache in this was the best tradeoff for caches whose sizes were in the KB range four way set associative cache fully associative cache the best lowest miss rates and so the best tradeoff when the miss penalty is very high Direct mapped cache edit Here each location in main memory can only go in one entry in the cache It does not have a replacement policy as such since there is no choice of which cache entry s contents to evict This means that if two locations map to the same entry they may continually knock each other out Although simpler a direct mapped cache needs to be much larger than an associative one to give comparable performance and is more unpredictable Let x be block number in cache y be block number of memory and n be number of blocks in cache then mapping is done with the help of the equation x y mod n Two way set associative cache edit If each location in main memory can be cached in either of two locations in the cache one logical question is which one of the two The simplest and most commonly used scheme shown in the right hand diagram above is to use the least significant bits of the memory location s index as the index for the cache memory and to have two entries for each index One benefit of this scheme is that the tags stored in the cache do not have to include that part of the main memory address which is implied by the cache memory s index Since the cache tags have fewer bits they take less area on the microprocessor chip and can be read and compared faster Also LRU is especially simple since only one bit needs to be stored for each pair Speculative execution edit One of the advantages of a direct mapped cache is that it allows simple and fast speculation Once the address has been computed the one cache index which might have a copy of that location in memory is known That cache entry can be read and the processor can continue to work with that data before it finishes checking that the tag actually matches the requested address The idea of having the processor use the cached data before the tag match completes can be applied to associative caches as well A subset of the tag called a hint can be used to pick just one of the possible cache entries mapping to the requested address The entry selected by the hint can then be used in parallel with checking the full tag The hint technique works best when used in the context of address translation as explained below Two way skewed associative cache edit Other schemes have been suggested such as the skewed cache where the index for way is direct as above but the index for way is formed with a hash function A good hash function has the property that addresses which conflict with the direct mapping tend not to conflict when mapped with the hash function and so it is less likely that a program will suffer from an unexpectedly large number of conflict misses due to a pathological access pattern The downside is extra latency from computing the hash function Additionally when it comes time to load a new line and evict an old line it may be difficult to determine which existing line was least recently used because the new line conflicts with data at different indexes in each way LRU tracking for non skewed caches is usually done on a per set basis Nevertheless skewed associative caches have major advantages over conventional set associative ones Pseudo associative cache edit A true set associative cache tests all the possible ways simultaneously using something like a content addressable memory A pseudo associative cache tests each possible way one at a time A hash rehash cache and a column associative cache are examples of a pseudo associative cache In the common case of finding a hit in the first way tested a pseudo associative cache is as fast as a direct mapped cache but it has a much lower conflict miss rate than a direct mapped cache closer to the miss rate of a fully associative cache Cache miss edit A cache miss refers to a failed attempt to read or write a piece of data in the cache which results in a main memory access with much longer latency There are three kinds of cache misses instruction read miss data read miss and data write miss A cache read miss from an instruction cache generally causes the most delay because the processor or at least the thread of execution has to wait stall until the instruction is fetched from main memory A cache read miss from a data cache usually causes less delay because instructions not dependent on the cache read can be issued and continue execution until the data is returned from main memory and the dependent instructions can resume execution A cache write miss to a data cache generally causes the least delay because the write can be queued and there are few limitations on the execution of subsequent instructions The processor can continue until the queue is full In order to lower cache miss rate a great deal of analysis has been done on cache behavior in an attempt to find the best combination of size associativity block size and so on Sequences of memory references performed by benchmark programs are saved as address traces Subsequent analyses simulate many different possible cache designs on these long address traces Making sense of how the many variables affect the cache hit rate can be quite confusing One significant contribution to this analysis was made by Mark Hill who separated misses into three categories known as the Three Cs Compulsory misses are those misses caused by the first reference to a location in memory Cache size and associativity make no difference to the number of compulsory misses Prefetching can help here as can larger cache block sizes which are a form of prefetching Compulsory misses are sometimes referred to as cold misses Capacity misses are those misses that occur regardless of associativity or block size solely due to the finite size of the cache The curve of capacity miss rate versus cache size gives some measure of the temporal locality of a particular reference stream Note that there is no useful notion of a cache being full or empty or near capacity CPU caches almost always have nearly every line filled with a copy of some line in main memory and nearly every allocation of a new line requires the eviction of an old line Conflict misses are those misses that could have been avoided had the cache not evicted an entry earlier Conflict misses can be further broken down into mapping misses that are unavoidable given a particular amount of associativity and replacement misses which are due to the particular victim choice of the replacement policy Miss rate versus cache size on the Integer portion of SPEC CPU The graph to the right summarizes the cache performance seen on the Integer portion of the SPEC CPU benchmarks as collected by Hill and Cantin These benchmarks are intended to represent the kind of workload that an engineering workstation computer might see on any given day The reader should keep in mind that finding benchmarks which are even usefully representative of many programs has been very difficult and there will always be important programs with very different behavior than what is shown here We can see the different effects of the three Cs in this graph At the far right with cache size labelled Inf we have the compulsory misses If we wish to improve a machine s performance on SpecInt increasing the cache size beyond MB is essentially futile That is the insight given by the compulsory misses The fully associative cache miss rate here is almost representative of the capacity miss rate The difference is that the data presented is from simulations assuming an LRU replacement policy Showing the capacity miss rate would require a perfect replacement policy i e an oracle that looks into the future to find a cache entry which is actually not going to be hit Note that our approximation of the capacity miss rate falls steeply between KB and KB This indicates that the benchmark has a working set of roughly KB A CPU cache designer examining this benchmark will have a strong incentive to set the cache size to KB rather than KB Note that on this benchmark no amount of associativity can make a KB cache perform as well as a KB way or even a direct mapped KB cache Finally note that between KB and MB there is a large difference between direct mapped and fully associative caches This difference is the conflict miss rate The insight from looking at conflict miss rates is that secondary caches benefit a great deal from high associativity This benefit was well known in the late s and early s when CPU designers could not fit large caches on chip and could not get sufficient bandwidth to either the cache data memory or cache tag memory to implement high associativity in off chip caches Desperate hacks were attempted the MIPS R used expensive off chip dedicated tag SRAMs which had embedded tag comparators and large drivers on the match lines in order to implement a MB four way associative cache The MIPS R used ordinary SRAM chips for the tags Tag access for both ways took two cycles To reduce latency the R would guess which way of the cache would hit on each access Address translation edit Most general purpose CPUs implement some form of virtual memory To summarize either each program running on the machine sees its own simplified address space which contains code and data for that program only or all programs run in a common virtual address space A program uses the virtual address space in which it runs without regard for where particular locations in that address space exist in physical memory Virtual memory requires the processor to translate virtual addresses generated by the program into physical addresses in main memory The portion of the processor that does this translation is known as the memory management unit MMU The fast path through the MMU can perform those translations stored in the translation lookaside buffer TLB which is a cache of mappings from the operating system s page table segment table or both For the purposes of the present discussion there are three important features of address translation Latency The physical address is available from the MMU some time perhaps a few cycles after the virtual address is available from the address generator Aliasing Multiple virtual addresses can map to a single physical address Most processors guarantee that all updates to that single physical address will happen in program order To deliver on that guarantee the processor must ensure that only one copy of a physical address resides in the cache at any given time Granularity The virtual address space is broken up into pages For instance a GB virtual address space might be cut up into pages of KB size each of which can be independently mapped There may be multiple page sizes supported see virtual memory for elaboration A historical note some early virtual memory systems were very slow because they required an access to the page table held in main memory before every programmed access to main memory NB With no caches this effectively cut the speed of the machine in half The first hardware cache used in a computer system was not actually a data or instruction cache but rather a TLB Caches can be divided into types based on whether the index or tag correspond to physical or virtual addresses Physically indexed physically tagged PIPT caches use the physical address for both the index and the tag While this is simple and avoids problems with aliasing it is also slow as the physical address must be looked up which could involve a TLB miss and access to main memory before that address can be looked up in the cache Virtually indexed virtually tagged VIVT caches use the virtual address for both the index and the tag This caching scheme can result in much faster lookups since the MMU does not need to be consulted first to determine the physical address for a given virtual address However VIVT suffers from aliasing problems where several different virtual addresses may refer to the same physical address The result is that such addresses would be cached separately despite referring to the same memory causing coherency problems Another problem is homonyms where the same virtual address maps to several different physical addresses It is not possible to distinguish these mappings by only looking at the virtual index though potential solutions include flushing the cache after a context switch forcing address spaces to be non overlapping tagging the virtual address with an address space ID ASID or using physical tags Additionally there is a problem that virtual to physical mappings can change which would require flushing cache lines as the VAs would no longer be valid Virtually indexed physically tagged VIPT caches use the virtual address for the index and the physical address in the tag The advantage over PIPT is lower latency as the cache line can be looked up in parallel with the TLB translation however the tag cannot be compared until the physical address is available The advantage over VIVT is that since the tag has the physical address the cache can detect homonyms VIPT requires more tag bits as the index bits no longer represent the same address Physically indexed virtually tagged PIVT caches are only theoretical as they would basically be useless A cache with this structure would be just as slow as PIPT suffering from aliasing problems at the same time like VIVT The speed of this recurrence the load latency is crucial to CPU performance and so most modern level caches are virtually indexed which at least allows the MMU s TLB lookup to proceed in parallel with fetching the data from the cache RAM But virtual indexing is not the best choice for all cache levels The cost of dealing with virtual aliases grows with cache size and as a result most level and larger caches are physically indexed Caches have historically used both virtual and physical addresses for the cache tags although virtual tagging is now uncommon If the TLB lookup can finish before the cache RAM lookup then the physical address is available in time for tag compare and there is no need for virtual tagging Large caches then tend to be physically tagged and only small very low latency caches are virtually tagged In recent general purpose CPUs virtual tagging has been superseded by vhints as described below Homonym and synonym problems edit The cache that relies on the virtual indexing and tagging becomes inconsistent after the same virtual address is mapped into different physical addresses homonym This can be solved by using physical address for tagging or by storing the address space id in the cache line However the latter of these two approaches does not help against the synonym problem where several cache lines end up storing data for the same physical address Writing to such location may update only one location in the cache leaving others with inconsistent data This problem might be solved by using non overlapping memory layouts for different address spaces or otherwise the cache or part of it must be flushed when the mapping changes Virtual tags and vhints edit The great advantage of virtual tags is that for associative caches they allow the tag match to proceed before the virtual to physical translation is done However coherence probes and evictions present a physical address for action The hardware must have some means of converting the physical addresses into a cache index generally by storing physical tags as well as virtual tags For comparison a physically tagged cache does not need to keep virtual tags which is simpler When a virtual to physical mapping is deleted from the TLB cache entries with those virtual addresses will have to be flushed somehow Alternatively if cache entries are allowed on pages not mapped by the TLB then those entries will have to be flushed when the access rights on those pages are changed in the page table It is also possible for the operating system to ensure that no virtual aliases are simultaneously resident in the cache The operating system makes this guarantee by enforcing page coloring which is described below Some early RISC processors SPARC RS took this approach It has not been used recently as the hardware cost of detecting and evicting virtual aliases has fallen and the software complexity and performance penalty of perfect page coloring has risen It can be useful to distinguish the two functions of tags in an associative cache they are used to determine which way of the entry set to select and they are used to determine if the cache hit or missed The second function must always be correct but it is permissible for the first function to guess and get the wrong answer occasionally Some processors e g early SPARCs have caches with both virtual and physical tags The virtual tags are used for way selection and the physical tags are used for determining hit or miss This kind of cache enjoys the latency advantage of a virtually tagged cache and the simple software interface of a physically tagged cache It bears the added cost of duplicated tags however Also during miss processing the alternate ways of the cache line indexed have to be probed for virtual aliases and any matches evicted The extra area and some latency can be mitigated by keeping virtual hints with each cache entry instead of virtual tags These hints are a subset or hash of the virtual tag and are used for selecting the way of the cache from which to get data and a physical tag Like a virtually tagged cache there may be a virtual hint match but physical tag mismatch in which case the cache entry with the matching hint must be evicted so that cache accesses after the cache fill at this address will have just one hint match Since virtual hints have fewer bits than virtual tags distinguishing them from one another a virtually hinted cache suffers more conflict misses than a virtually tagged cache Perhaps the ultimate reduction of virtual hints can be found in the Pentium Willamette and Northwood cores In these processors the virtual hint is effectively bits and the cache is way set associative Effectively the hardware maintains a simple permutation from virtual address to cache index so that no content addressable memory CAM is necessary to select the right one of the four ways fetched Page coloring edit Main article Cache coloring Large physically indexed caches usually secondary caches run into a problem the operating system rather than the application controls which pages collide with one another in the cache Differences in page allocation from one program run to the next lead to differences in the cache collision patterns which can lead to very large differences in program performance These differences can make it very difficult to get a consistent and repeatable timing for a benchmark run To understand the problem consider a CPU with a MB physically indexed direct mapped level cache and KB virtual memory pages Sequential physical pages map to sequential locations in the cache until after pages the pattern wraps around We can label each physical page with a color of to denote where in the cache it can go Locations within physical pages with different colors cannot conflict in the cache Programmers attempting to make maximum use of the cache may arrange their programs access patterns so that only MB of data need be cached at any given time thus avoiding capacity misses But they should also ensure that the access patterns do not have conflict misses One way to think about this problem is to divide up the virtual pages the program uses and assign them virtual colors in the same way as physical colors were assigned to physical pages before Programmers can then arrange the access patterns of their code so that no two pages with the same virtual color are in use at the same time There is a wide literature on such optimizations e g loop nest optimization largely coming from the High Performance Computing HPC community The snag is that while all the pages in use at any given moment may have different virtual colors some may have the same physical colors In fact if the operating system assigns physical pages to virtual pages randomly and uniformly it is extremely likely that some pages will have the same physical color and then locations from those pages will collide in the cache this is the birthday paradox The solution is to have the operating system attempt to assign different physical color pages to different virtual colors a technique called page coloring Although the actual mapping from virtual to physical color is irrelevant to system performance odd mappings are difficult to keep track of and have little benefit so most approaches to page coloring simply try to keep physical and virtual page colors the same If the operating system can guarantee that each physical page maps to only one virtual color then there are no virtual aliases and the processor can use virtually indexed caches with no need for extra virtual alias probes during miss handling Alternatively the O S can flush a page from the cache whenever it changes from one virtual color to another As mentioned above this approach was used for some early SPARC and RS designs Cache hierarchy in a modern processor edit Memory hierarchy of an AMD Bulldozer server Modern processors have multiple interacting caches on chip The operation of a particular cache can be completely specified by the cache size the cache block size the number of blocks in a set the cache set replacement policy the cache write policy write through or write back While all of the cache blocks in a particular cache are the same size and have the same associativity typically lower level caches such as the L cache have a smaller size have smaller blocks and have fewer blocks in a set while higher level caches such as the L cache have larger size larger blocks and more blocks in a set Specialized caches edit Pipelined CPUs access memory from multiple points in the pipeline instruction fetch virtual to physical address translation and data fetch see classic RISC pipeline The natural design is to use different physical caches for each of these points so that no one physical resource has to be scheduled to service two points in the pipeline Thus the pipeline naturally ends up with at least three separate caches instruction TLB and data each specialized to its particular role Victim cache edit A victim cache is a cache used to hold blocks evicted from a CPU cache upon replacement The victim cache lies between the main cache and its refill path and only holds blocks that were evicted from the main cache The victim cache is usually fully associative and is intended to reduce the number of conflict misses Many commonly used programs do not require an associative mapping for all the accesses In fact only a small fraction of the memory accesses of the program require high associativity The victim cache exploits this property by providing high associativity to only these accesses It was introduced by Norman Jouppi from DEC in Intel s Crystal Well variant of its Haswell processors equipped with Intel s Iris Pro GT e embedded graphics and MB of eDRAM introduced an on package Level cache which serves as a victim cache to the processors s Level cache Trace cache edit One of the more extreme examples of cache specialization is the trace cache found in the Intel Pentium microprocessors A trace cache is a mechanism for increasing the instruction fetch bandwidth and decreasing power consumption in the case of the Pentium by storing traces of instructions that have already been fetched and decoded The earliest widely acknowledged academic publication of trace cache was by Eric Rotenberg Steve Bennett and Jim Smith in their paper Trace Cache a Low Latency Approach to High Bandwidth Instruction Fetching An earlier publication is US patent Alex Peleg and Uri Weiser of Intel Corp Dynamic flow instruction cache memory organized around trace segments independent of virtual address line a continuation of an application filed in later abandoned A trace cache stores instructions either after they have been decoded or as they are retired Generally instructions are added to trace caches in groups representing either individual basic blocks or dynamic instruction traces A dynamic trace trace path contains only instructions whose results are actually used and eliminates instructions following taken branches since they are not executed a dynamic trace can be a concatenation of multiple basic blocks This allows the instruction fetch unit of a processor to fetch several basic blocks without having to worry about branches in the execution flow Trace lines are stored in the trace cache based on the program counter of the first instruction in the trace and a set of branch predictions This allows for storing different trace paths that start on the same address each representing different branch outcomes In the instruction fetch stage of a pipeline the current program counter along with a set of branch predictions is checked in the trace cache for a hit If there is a hit a trace line is supplied to fetch which does not have to go to a regular cache or to memory for these instructions The trace cache continues to feed the fetch unit until the trace line ends or until there is a misprediction in the pipeline If there is a miss a new trace starts to be built The Pentium s trace cache stores micro operations resulting from decoding x instructions providing also the functionality of a micro operation cache Having this the next time an instruction is needed it does not have to be decoded into micro ops again Micro operation uop cache edit A micro operation cache Uop Cache UC is a specialized cache that stores micro operations of decoded instructions as received directly from the instruction decoders or from the instruction cache When an instruction needs to be decoded the uop cache is checked for its decoded form which is re used if cached if it is not available the instruction is decoded and then cached One of the early works describing uop cache as an alternative frontend for the Intel P processor family is the paper Micro Operation Cache A Power Aware Frontend for Variable Instruction Length ISA Later Intel included uop caches in its Sandy Bridge processors and in successive microarchitectures like Ivy Bridge and Haswell Fetching complete pre decoded instructions eliminates the need to repeatedly decode variable length complex instructions into simpler fixed length micro operations and simplifies the process of predicting fetching rotating and aligning fetched instructions A uop cache effectively offloads the fetch and decode hardware thus decreasing power consumption and improving the frontend supply of decoded micro operations The uop cache also increases performance by more consistently delivering decoded micro operations to the backend and eliminating various bottlenecks in the CPU s fetch and decode logic A uop cache has many similarities with a trace cache although a uop cache is much simpler thus providing better power efficiency The main disadvantage of the trace cache leading to its power inefficiency is the hardware complexity required for its heuristic deciding on caching and reusing dynamically created instruction traces Multi level caches edit Another issue is the fundamental tradeoff between cache latency and hit rate Larger caches have better hit rates but longer latency To address this tradeoff many computers use multiple levels of cache with small fast caches backed up by larger slower caches Multi level caches generally operate by checking the fastest level L cache first if it hits the processor proceeds at high speed If that smaller cache misses the next fastest cache level L is checked and so on before external memory is checked As the latency difference between main memory and the fastest cache has become larger some processors have begun to utilize as many as three levels of on chip cache Price sensitive designs used this to pull the entire cache hierarchy on chip but by the s some of the highest performance designs returned to having large off chip caches often implemented in eDRAM and mounted on a multi chip module as a fourth cache level For example the Alpha had to MB off chip L cache the IBM POWER had off chip L caches of MB per processor shared among several processors the Itanium had a MB unified level L cache on die the Itanium MX Module incorporates two Itanium processors along with a shared MB L cache on a Multi chip module that was pin compatible with a Madison processor Intel s Xeon MP product code named Tulsa features MB of on die L cache shared between two processor cores the AMD Phenom II has up to MB on die unified L cache the Intel Core i has an MB on die unified L cache that is inclusive shared by all cores Intel Haswell CPUs with integrated Intel Iris Pro Graphics have MB of eDRAM acting essentially as an L cache The benefits of an L cache depend on the application s access patterns Finally at the other end of the memory hierarchy the CPU register file itself can be considered the smallest fastest cache in the system with the special characteristic that it is scheduled in software typically by a compiler as it allocates registers to hold values retrieved from main memory See especially loop nest optimization Register files sometimes also have hierarchy The Cray circa had eight address A and eight scalar data S registers that were generally usable There was also a set of address B and scalar data T registers that took longer to access but were faster than main memory The B and T registers were provided because the Cray did not have a data cache The Cray did however have an instruction cache Multi core chips edit When considering a chip with multiple cores there is a question of whether the caches should be shared or local to each core Implementing shared cache undoubtedly introduces more wiring and complexity But then having one cache per chip rather than core greatly reduces the amount of space needed and thus one can include a larger cache Typically sharing the L cache is undesirable since the latency increase is such that each core will run considerably slower than a single core chip On the other side for the highest level the last one called before accessing memory having a global cache is desirable for several reasons For example an eight core chip with three levels may include an L cache for each core one intermediate L cache for each pair of cores and one L cache shared by all cores Shared highest level cache which is called before accessing memory is usually referred to as the last level cache LLC Additional techniques are used for increasing the level of parallelism when LLC is shared between multiple cores including slicing it into multiple pieces which are addressing certain ranges of memory addresses and can be accessed independently Separate versus unified edit In a separate cache structure instructions and data are cached separately meaning that a cache line is used to cache either instructions or data but not both various benefits have been demonstrated with separate data and instruction translation lookaside buffers In a unified structure this constraint is not present and cache lines can be used to cache both instructions and data Exclusive versus inclusive edit Multi level caches introduce new design decisions For instance in some processors all data in the L cache must also be somewhere in the L cache These caches are called strictly inclusive Other processors like the AMD Athlon have exclusive caches data is guaranteed to be in at most one of the L and L caches never in both Still other processors like the Intel Pentium II III and do not require that data in the L cache also reside in the L cache although it may often do so There is no universally accepted name for this intermediate policy The advantage of exclusive caches is that they store more data This advantage is larger when the exclusive L cache is comparable to the L cache and diminishes if the L cache is many times larger than the L cache When the L misses and the L hits on an access the hitting cache line in the L is exchanged with a line in the L This exchange is quite a bit more work than just copying a line from L to L which is what an inclusive cache does One advantage of strictly inclusive caches is that when external devices or other processors in a multiprocessor system wish to remove a cache line from the processor they need only have the processor check the L cache In cache hierarchies which do not enforce inclusion the L cache must be checked as well As a drawback there is a correlation between the associativities of L and L caches if the L cache does not have at least as many ways as all L caches together the effective associativity of the L caches is restricted Another disadvantage of inclusive cache is that whenever there is an eviction in L cache the possibly corresponding lines in L also have to get evicted in order to maintain inclusiveness This is quite a bit work and would result in higher L miss rate Another advantage of inclusive caches is that the larger cache can use larger cache lines which reduces the size of the secondary cache tags Exclusive caches require both caches to have the same size cache lines so that cache lines can be swapped on a L miss L hit If the secondary cache is an order of magnitude larger than the primary and the cache data is an order of magnitude larger than the cache tags this tag area saved can be comparable to the incremental area needed to store the L cache data in the L Example the K edit To illustrate both specialization and multi level caching here is the cache hierarchy of the K core in the AMD Athlon CPU Cache hierarchy of the K core in the AMD Athlon CPU The K has specialized caches an instruction cache an instruction TLB a data TLB and a data cache Each of these caches is specialized The instruction cache keeps copies of byte lines of memory and fetches bytes each cycle Each byte in this cache is stored in ten bits rather than with the extra bits marking the boundaries of instructions this is an example of predecoding The cache has only parity protection rather than ECC because parity is smaller and any damaged data can be replaced by fresh data fetched from memory which always has an up to date copy of instructions The instruction TLB keeps copies of page table entries PTEs Each cycle s instruction fetch has its virtual address translated through this TLB into a physical address Each entry is either or bytes in memory Because the K has a variable page size each of the TLBs is split into two sections one to keep PTEs that map KB pages and one to keep PTEs that map MB or MB pages The split allows the fully associative match circuitry in each section to be simpler The operating system maps different sections of the virtual address space with different size PTEs The data TLB has two copies which keep identical entries The two copies allow two data accesses per cycle to translate virtual addresses to physical addresses Like the instruction TLB this TLB is split into two kinds of entries The data cache keeps copies of byte lines of memory It is split into banks each storing KB of data and can fetch two byte data each cycle so long as those data are in different banks There are two copies of the tags because each byte line is spread among all banks Each tag copy handles one of the two accesses per cycle The K also has multiple level caches There are second level instruction and data TLBs which store only PTEs mapping KB Both instruction and data caches and the various TLBs can fill from the large unified L cache This cache is exclusive to both the L instruction and data caches which means that any byte line can only be in one of the L instruction cache the L data cache or the L cache It is however possible for a line in the data cache to have a PTE which is also in one of the TLBs the operating system is responsible for keeping the TLBs coherent by flushing portions of them when the page tables in memory are updated The K also caches information that is never stored in memory prediction information These caches are not shown in the above diagram As is usual for this class of CPU the K has fairly complex branch prediction with tables that help predict whether branches are taken and other tables which predict the targets of branches and jumps Some of this information is associated with instructions in both the level instruction cache and the unified secondary cache The K uses an interesting trick to store prediction information with instructions in the secondary cache Lines in the secondary cache are protected from accidental data corruption e g by an alpha particle strike by either ECC or parity depending on whether those lines were evicted from the data or instruction primary caches Since the parity code takes fewer bits than the ECC code lines from the instruction cache have a few spare bits These bits are used to cache branch prediction information associated with those instructions The net result is that the branch predictor has a larger effective history table and so has better accuracy More hierarchies edit Other processors have other kinds of predictors e g the store to load bypass predictor in the DEC Alpha and various specialized predictors are likely to flourish in future processors These predictors are caches in that they store information that is costly to compute Some of the terminology used when discussing predictors is the same as that for caches one speaks of a hit in a branch predictor but predictors are not generally thought of as part of the cache hierarchy The K keeps the instruction and data caches coherent in hardware which means that a store into an instruction closely following the store instruction will change that following instruction Other processors like those in the Alpha and MIPS family have relied on software to keep the instruction cache coherent Stores are not guaranteed to show up in the instruction stream until a program calls an operating system facility to ensure coherency Implementation edit Main article Cache algorithms Cache reads are the most common CPU operation that takes more than a single cycle Program execution time tends to be very sensitive to the latency of a level data cache hit A great deal of design effort and often power and silicon area are expended making the caches as fast as possible The simplest cache is a virtually indexed direct mapped cache The virtual address is calculated with an adder the relevant portion of the address extracted and used to index an SRAM which returns the loaded data The data is byte aligned in a byte shifter and from there is bypassed to the next operation There is no need for any tag checking in the inner loop in fact the tags need not even be read Later in the pipeline but before the load instruction is retired the tag for the loaded data must be read and checked against the virtual address to make sure there was a cache hit On a miss the cache is updated with the requested cache line and the pipeline is restarted An associative cache is more complicated because some form of tag must be read to determine which entry of the cache to select An N way set associative level cache usually reads all N possible tags and N data in parallel and then chooses the data associated with the matching tag Level caches sometimes save power by reading the tags first so that only one data element is read from the data SRAM Read path for a way associative cache The diagram to the right is intended to clarify the manner in which the various fields of the address are used Address bit is most significant bit is least significant The diagram shows the SRAMs indexing and multiplexing for a KB way set associative virtually indexed and virtually tagged cache with byte B lines a bit read width and bit virtual address Because the cache is KB and has B lines there are just lines in the cache and we read two at a time from a Tag SRAM which has rows each with a pair of bit tags Although any function of virtual address bits through could be used to index the tag and data SRAMs it is simplest to use the least significant bits Similarly because the cache is KB and has a B read path and reads two ways for each access the Data SRAM is rows by bytes wide A more modern cache might be KB way set associative virtually indexed virtually hinted and physically tagged with B lines bit read width and bit physical addresses The read path recurrence for such a cache looks very similar to the path above Instead of tags vhints are read and matched against a subset of the virtual address Later on in the pipeline the virtual address is translated into a physical address by the TLB and the physical tag is read just one as the vhint supplies which way of the cache to read Finally the physical address is compared to the physical tag to determine if a hit has occurred Some SPARC designs have improved the speed of their L caches by a few gate delays by collapsing the virtual address adder into the SRAM decoders See Sum addressed decoder History edit The early history of cache technology is closely tied to the invention and use of virtual memory citation needed Because of scarcity and cost of semi conductors memories early mainframe computers in the s used a complex hierarchy of physical memory mapped onto a flat virtual memory space used by programs The memory technologies would span semi conductor magnetic core drum and disc Virtual memory seen and used by programs would be flat and caching would be used to fetch data and instructions into the fastest memory ahead of processor access Extensive studies were done to optimize the cache sizes Optimal values were found to depend greatly on the programming language used with Algol needing the smallest and Fortran and Cobol needing the largest cache sizes disputed discuss In the early days of microcomputer technology memory access was only slightly slower than register access But since the s the performance gap between processor and memory has been growing Microprocessors have advanced much faster than memory especially in terms of their operating frequency so memory became a performance bottleneck While it was technically possible to have all the main memory as fast as the CPU a more economically viable path has been taken use plenty of low speed memory but also introduce a small high speed cache memory to alleviate the performance gap This provided an order of magnitude more capacity for the same price with only a slightly reduced combined performance First TLB implementations edit The first documented uses of a TLB were on the GE and the IBM both of which used an associative memory as a TLB First data cache edit The first documented use of a data cache was on the IBM System Model In x microprocessors edit As the x microprocessors reached clock rates of MHz and above in the small amounts of fast cache memory began to be featured in systems to improve performance This was because the DRAM used for main memory had significant latency up to ns as well as refresh cycles The cache was constructed from more expensive but significantly faster SRAM which at the time had latencies around ns The early caches were external to the processor and typically located on the motherboard in the form of eight or nine DIP devices placed in sockets to enable the cache as an optional extra or upgrade feature Some versions of the Intel processor could support to KB of external cache With the processor an KB cache was integrated directly into the CPU die This cache was termed Level or L cache to differentiate it from the slower on motherboard or Level L cache These on motherboard caches were much larger with the most common size being KB The popularity of on motherboard cache continued on through the Pentium MMX era but was made obsolete by the introduction of SDRAM and the growing disparity between bus clock rates and CPU clock rates which caused on motherboard cache to be only slightly faster than main memory The next development in cache implementation in the x microprocessors began with the Pentium Pro which brought the secondary cache onto the same package as the microprocessor clocked at the same frequency as the microprocessor On motherboard caches enjoyed prolonged popularity thanks to the AMD K and AMD K III processors that still used the venerable Socket which was previously used by Intel with on motherboard caches K III included KB on die L cache and took advantage of the on board cache as a third level cache named L motherboards with up to MB of on board cache were produced After the Socket became obsolete on motherboard cache disappeared from the x systems The three level caches were used again first with the introduction of multiple processor cores where the L cache was added to the CPU die It became common for the total cache sizes to be increasingly larger in newer processor generations and recently as of it is not uncommon to find Level cache sizes of tens of megabytes This trend appears to continue for the foreseeable future Intel introduced a Level on package cache with the Haswell microarchitecture Crystal Well Haswell CPUs equipped with the GT e variant of Intel s integrated Iris Pro graphics effectively feature MB of embedded DRAM eDRAM on the same package This L cache is shared dynamically between the on die GPU and CPU and serves as a victim cache to the CPU s L cache Current research edit Early cache designs focused entirely on the direct cost of cache and RAM and average execution speed More recent cache designs also consider energy efficiency fault tolerance and other goals There are several tools available to computer architects to help explore tradeoffs between cache cycle time energy and area These tools include the open source CACTI cache simulator and the open source SimpleScalar instruction set simulator Multi ported cache edit A multi ported cache is a cache which can serve more than one request at a time When accessing a traditional cache we normally use a single memory address whereas in a multi ported cache we may request N addresses at a time where N is the number of ports that connected through the processor and the cache The benefit of this is that a pipelined processor may access memory from different phases in its pipeline Another benefit is that it allows the concept of super scalar processors through different cache levels See also edit Cache coherency Cache algorithms Dinero Cache simulator by University of Wisconsin System Instruction unit Memoization briefly defined in List of computer term etymologies No write allocation Scratchpad RAM Write buffer Notes edit The very first paging machine the Ferranti Atlas had no page tables in main memory there was an associative memory with one entry for every word page frame of core References edit John L Hennessy David A Patterson Computer Architecture A Quantitative Approach ISBN X ISBN page B David A Patterson John L Hennessy Computer organization and design the hardware software interface ISBN ISBN Chapter Large and Fast Exploiting the Memory Hierarchy p a b c Gene Cooperman Cache Basics Ben Dugan Concerning Caches Harvey G Cragon Memory systems and pipelined processors ISBN ISBN Chapter Cache Addressing Virtual or Real p Cache design PDF ucsd edu p Retrieved IEEE Xplore Phased set associative cache design for reduced power consumption Ieeexplore ieee org Retrieved on a b Andr Seznec A Case for Two Way Skewed Associative Caches doi Retrieved a b Advanced Caching Techniques by C Kozyrakis Micro Architecture Skewed associative caches have major advantages over conventional set associative caches Cache performance of SPEC CPU Cs wisc edu Retrieved Sumner F H Haley G Chenh E C Y The Central Control Unit of the Atlas Computer Information Processing IFIP Congress Proceedings Proceedings of IFIP Congress Spartan Kilburn T Payne R B Howarth D J December The Atlas Supervisor Computers Key to Total Systems Control Conferences Proceedings Proceedings of the Eastern Joint Computer Conference Washington D C Macmillan pp Understanding Caching Linux Journal Retrieved dead link N P Jouppi Improving direct mapped cache performance by the addition of a small fully associative cache and prefetch buffers th Annual International Symposium on Computer Architecture Proceedings doi ISCA a b Products Formerly Crystal Well Intel Retrieved a b Intel Iris Pro Graphics Review Core i HQ Tested AnandTech Retrieved Trace Cache a Low Latency Approach to High Bandwidth Instruction Fetching doi MICRO a b Leon Gu Dipti Motiani October Trace Cache PDF Retrieved a b Agner Fog The microarchitecture of Intel AMD and VIA CPUs An optimization guide for assembly programmers and compiler makers PDF agner org Retrieved a b Baruch Solomon Avi Mendelson Doron Orenstein Yoav Almog Ronny Ronen August Micro Operation Cache A Power Aware Frontend for Variable Instruction Length ISA PDF Intel doi LPE Retrieved a b Anand Lal Shimpi Intel s Haswell Architecture Analyzed AnandTech Retrieved Intel Iris Pro Graphics Review Core i HQ Tested AnandTech Retrieved Oded Lempel nd Generation Intel Core Processor Family Intel Core i i and i PDF hotchips org p Retrieved Chen J Bradley Borg Anita Jouppi Norman P A Simulation Based Study of TLB Performance SIGARCH Computer Architecture News doi AMD K Sandpile org Retrieved Mahapatra Nihar R Venkatrao Balakrishna The processor memory bottleneck problems and solutions Association of Computing Machinery doi Retrieved GE January GE System Manual IBM February IBM System Model Functional Characteristics Third Edition GA IBM June IBM System Model Functional Characteristics SECOND EDITION A Intel Xeon Processor E Family Intel Retrieved Chip Design Thwarts Sneak Attack on Data by Sally Adee discusses A novel cache architecture with enhanced performance and security by Zhenghong Wang and Ruby B Lee abstract Caches ideally should have low miss rates and short access times and should be power efficient at the same time Such design goals are often contradictory in practice CACTI Hpl hp com Retrieved External links edit The Wikibook Microprocessor Design has a page on the topic of Cache Memory part CPU caches An article on lwn net by Ulrich Drepper describing CPU caches in detail Evaluating Associativity in CPU Caches Hill and Smith Introduces capacity conflict and compulsory classification Cache Performance for SPEC CPU Benchmarks Hill and Cantin This reference paper has been updated several times It has thorough and lucidly presented simulation results for a reasonably wide set of benchmarks and cache organizations Memory Hierarchy in Cache Based Systems by Ruud van der Pas Sun Microsystems is a nice introductory article to CPU memory caching A Cache Primer by Paul Genua P E Freescale Semiconductor another introductory article An way set associative cache written in VHDL Understanding CPU caching and performance An article on Ars Technica by Jon Stokes IBM POWER processor review An article on ixbtlabs by Pavel Danilov v t e CPU technologies Architecture Harvard Modified Harvard von Neumann Dataflow Comparison Instruction set ASIP CISC EDGE EPIC MISC OISC RISC VLIW NISC ZISC Word size bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit bit variable Pipeline Instruction pipelining In order amp out of order execution Branch predictor Register renaming Speculative execution Hazards Bubble Parallel level Bit Bit serial Word Instruction Scalar Superscalar Data Vector Task Thread Multithreading Temporal multithreading Simultaneous multithreading Hyper threading Flynn s taxonomy SISD SIMD MISD MIMD SPMD Types Digital signal processor GPGPU Microcontroller Physics processing unit System on a chip Cellular Components ALU Barrel shifter Floating point unit Back side bus Multiplexer Demultiplexer Registers Memory management unit TLB Cache Register file Microcode Control unit Clock rate Power management APM ACPI Dynamic frequency scaling Dynamic voltage scaling Clock gating Retrieved from http en wikipedia org w index php title CPU cache amp oldid Categories Central processing unit Computer memory Cache computing Hidden categories All articles with dead external links Articles with dead external links from July Articles to be merged from July All articles to be merged All articles with unsourced statements Articles with unsourced statements from March All accuracy disputes Articles with disputed statements from December Navigation menu Personal tools Create account Log in Namespaces Article Talk Variants Views Read Edit View history Actions Search Navigation Main page Contents Featured content Current events Random article Donate to Wikipedia Wikimedia Shop Interaction Help About Wikipedia Community portal Recent changes Contact page Tools What links here Related changes Upload file Special pages Permanent link Page information Data item Cite this page Print export Create a book Download as PDF Printable version Languages Eesti Gaeilge Hrvatski Bahasa Indonesia Italiano Latvie u Magyar Norsk bokm l Polski Portugu s Sloven ina srpski Edit links This page was last modified on March at Text is available under the Creative Commons Attribution ShareAlike License additional terms may apply By using this site you agree to the Terms of Use and Privacy Policy Wikipedia is a registered trademark of the Wikimedia Foundation Inc a non profit organization Privacy policy About Wikipedia Disclaimers Contact Wikipedia Developers Mobile view if window mw mw loader state site loading user ready user groups ready if window mw mw loader load ext cite mobile desktop mediawiki action view postEdit mediawiki user mediawiki hidpi mediawiki page ready mediawiki searchSuggest ext gadget teahouse ext gadget ReferenceTooltips ext gadget DRN wizard ext gadget charinsert mw MwEmbedSupport style ext navigationTiming schema UniversalLanguageSelector ext uls eventlogger ext uls interlanguage skins vector collapsibleNav null true 